{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a940a831",
   "metadata": {},
   "source": [
    "Objectifs de ce TP:\n",
    "- Comprendre les Ã©tapes essentielles du prÃ©traitement de texte\n",
    "- MaÃ®triser les techniques de nettoyage et normalisation\n",
    "- Appliquer la rÃ©duction linguistique (stemming, lemmatisation)\n",
    "- DÃ©couvrir briÃ¨vement la transformation numÃ©rique\n",
    "\n",
    "PrÃ©requis: Connaissance de base en Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5a13ef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2949.60s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: spacy in ./.venv/lib/python3.13/site-packages (3.8.11)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (2.4.1)\n",
      "Requirement already satisfied: emoji in ./.venv/lib/python3.13/site-packages (2.15.0)\n",
      "Requirement already satisfied: contractions in ./.venv/lib/python3.13/site-packages (0.1.73)\n",
      "Requirement already satisfied: langdetect in ./.venv/lib/python3.13/site-packages (1.0.9)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.13/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.13/site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.13/site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.13/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.13/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.13/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./.venv/lib/python3.13/site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.13/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.13/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.13/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in ./.venv/lib/python3.13/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from spacy) (0.21.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.13/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.13/site-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in ./.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in ./.venv/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.13/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.13/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in ./.venv/lib/python3.13/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.13/site-packages (from langdetect) (1.17.0)\n",
      "Requirement already satisfied: anyascii in ./.venv/lib/python3.13/site-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
      "Requirement already satisfied: pyahocorasick in ./.venv/lib/python3.13/site-packages (from textsearch>=0.0.21->contractions) (2.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->spacy) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2955.95s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2967.22s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# INSTALLATION DES BIBLIOTHÃˆQUES NÃ‰CESSAIRES\n",
    "# ==========================================\n",
    "\n",
    "# DÃ©commentez et exÃ©cutez cette cellule si nÃ©cessaire\n",
    "!pip install nltk spacy pandas numpy emoji contractions langdetect\n",
    "!python -m spacy download fr_core_news_sm\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# ==========================================\n",
    "# IMPORTS\n",
    "# ==========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a660cd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/angelo-\n",
      "[nltk_data]     btma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/angelo-\n",
      "[nltk_data]     btma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/angelo-\n",
      "[nltk_data]     btma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/angelo-btma/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/angelo-\n",
      "[nltk_data]     btma/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Toutes les bibliothÃ¨ques sont chargÃ©es avec succÃ¨s!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# TÃ©lÃ©chargement des ressources NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Imports spÃ©cifiques\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Chargement des modÃ¨les spaCy\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"âœ… Toutes les bibliothÃ¨ques sont chargÃ©es avec succÃ¨s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd63a7",
   "metadata": {},
   "source": [
    "### **PARTIE 1: NETTOYAGE DE BASE (CLEANING)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eceaefd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARTIE 1: NETTOYAGE DE BASE\n",
      "============================================================\n",
      "Texte original:\n",
      "\n",
      "    Bonjour   tout le monde!!!  ğŸ˜€ \n",
      "    Ceci est un exemple de TEXTE Ã  nettoyer...   \n",
      "\n",
      "    Il contient des    espaces   multiples, des MAJUSCULES, \n",
      "    des chiffres comme 2024 et 12345, des URLs comme https://example.com,\n",
      "    et mÃªme des balises HTML <p>comme ceci</p>.\n",
      "\n",
      "    On peut aussi trouver des emails: contact@example.com\n",
      "    Et des Ã©moticÃ´nes :-)  :-(\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARTIE 1: NETTOYAGE DE BASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Exemple de texte brut avec plusieurs problÃ¨mes\n",
    "texte_brut = \"\"\"\n",
    "    Bonjour   tout le monde!!!  ğŸ˜€ \n",
    "    Ceci est un exemple de TEXTE Ã  nettoyer...   \n",
    "    \n",
    "    Il contient des    espaces   multiples, des MAJUSCULES, \n",
    "    des chiffres comme 2024 et 12345, des URLs comme https://example.com,\n",
    "    et mÃªme des balises HTML <p>comme ceci</p>.\n",
    "    \n",
    "    On peut aussi trouver des emails: contact@example.com\n",
    "    Et des Ã©moticÃ´nes :-)  :-(\n",
    "\"\"\"\n",
    "\n",
    "print(\"Texte original:\")\n",
    "print(texte_brut)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b2841812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Suppression des balises HTML\n",
    "def supprimer_html(texte):\n",
    "    \"\"\"Supprime les balises HTML du texte\"\"\"\n",
    "    return re.sub(r'<[^>]+>', '', texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f2328fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Bonjour   tout le monde!!!  ğŸ˜€ \n",
      "    Ceci est un exemple de TEXTE Ã  nettoyer...   \n",
      "\n",
      "    Il contient des    espaces   multiples, des MAJUSCULES, \n",
      "    des chiffres comme 2024 et 12345, des URLs comme https://example.com,\n",
      "    et mÃªme des balises HTML comme ceci.\n",
      "\n",
      "    On peut aussi trouver des emails: contact@example.com\n",
      "    Et des Ã©moticÃ´nes :-)  :-(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remplacer texte (argument de la fonction par notre texte brute)\n",
    "texte_brut = supprimer_html(texte_brut)\n",
    "print(texte_brut)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "580e15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Suppression des URLs\n",
    "def supprimer_urls(texte):\n",
    "    \"\"\"Supprime les URLs du texte\"\"\"\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ca7e314e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Bonjour   tout le monde!!!  ğŸ˜€ \n",
      "    Ceci est un exemple de TEXTE Ã  nettoyer...   \n",
      "\n",
      "    Il contient des    espaces   multiples, des MAJUSCULES, \n",
      "    des chiffres comme 2024 et 12345, des URLs comme \n",
      "    et mÃªme des balises HTML comme ceci.\n",
      "\n",
      "    On peut aussi trouver des emails: contact@example.com\n",
      "    Et des Ã©moticÃ´nes :-)  :-(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remplacer texte (argument de la fonction par notre texte brute)\n",
    "texte_brut = supprimer_urls(texte_brut)\n",
    "print(texte_brut)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5122e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Suppression des emails\n",
    "def supprimer_emails(texte):\n",
    "    \"\"\"Supprime les adresses email - MÃ©thode regex\"\"\"\n",
    "    return re.sub(r'\\S+@\\S+', '', texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "43641151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Bonjour   tout le monde!!!  ğŸ˜€ \n",
      "    Ceci est un exemple de TEXTE Ã  nettoyer...   \n",
      "\n",
      "    Il contient des    espaces   multiples, des MAJUSCULES, \n",
      "    des chiffres comme 2024 et 12345, des URLs comme \n",
      "    et mÃªme des balises HTML comme ceci.\n",
      "\n",
      "    On peut aussi trouver des emails: \n",
      "    Et des Ã©moticÃ´nes :-)  :-(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remplacer texte (argument de la fonction par notre texte brute)\n",
    "texte_brut = supprimer_emails(texte_brut)\n",
    "print(texte_brut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6ecc584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************************************\n",
      "APPROCHE PROFESSIONNELLE: NETTOYAGE AVEC LISTE DE FILTRAGE\n",
      "************************************************************\n",
      "Email testexamplecom  Prix 9999  Code ABC123  et balise div\n"
     ]
    }
   ],
   "source": [
    "# AUTRE MÃ‰THODE PROFESSIONNELLE POUR NETTOYAGE GÃ‰NÃ‰RAL\n",
    "print(\"\\n\" + \"*\"*60)\n",
    "print(\"APPROCHE PROFESSIONNELLE: NETTOYAGE AVEC LISTE DE FILTRAGE\")\n",
    "print(\"*\"*60)\n",
    "\n",
    "def nettoyer_texte_liste_filtrage(texte):\n",
    "    \"\"\"\n",
    "    MÃ©thode trÃ¨s utilisÃ©e en entreprise: dÃ©finir ce qu'on GARDE\n",
    "    plutÃ´t que ce qu'on supprime.\n",
    "    \n",
    "    Principe: On dÃ©finit les caractÃ¨res autorisÃ©s (whitelist)\n",
    "    \"\"\"\n",
    "    # Liste des caractÃ¨res autorisÃ©s (lettres, chiffres, espaces)\n",
    "    caracteres_autorises = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 Ã Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã¼Ã¿Ã§Ã€Ã‚Ã„Ã‰ÃˆÃŠÃ‹ÃÃÃ”Ã™Ã›ÃœÅ¸Ã‡')\n",
    "    \n",
    "    # On garde seulement ce qui est autorisÃ©\n",
    "    return ''.join([char for char in texte if char in caracteres_autorises])\n",
    "\n",
    "\n",
    "# Remplacer texte (argument de la fonction par notre texte brute)\n",
    "texte_sale = \"Email: test@example.com | Prix: 99.99â‚¬ | Code: ABC-123 !!! et balise </div>\"\n",
    "texte_sale = nettoyer_texte_liste_filtrage(texte_sale)\n",
    "print(texte_sale)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "10c6bf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email testexamplecom  Prix â‚¬  Code ABC  et balise div\n"
     ]
    }
   ],
   "source": [
    "# Ou inversement: dÃ©finir ce qu'on SUPPRIME (blacklist)\n",
    "def nettoyer_texte_blacklist(texte):\n",
    "    \"\"\"\n",
    "    Alternative: liste noire de caractÃ¨res Ã  supprimer\n",
    "    Plus flexible pour des cas spÃ©cifiques\n",
    "    \"\"\"\n",
    "    # Liste des caractÃ¨res Ã  Ã©liminer\n",
    "    caracteres_interdits = set(string.punctuation + '0123456789')\n",
    "    \n",
    "    # On garde tout SAUF les interdits\n",
    "    return ''.join([char for char in texte if char not in caracteres_interdits])\n",
    "\n",
    "# Comparaison\n",
    "texte_sale = \"Email: test@example.com | Prix: 99.99â‚¬ | Code: ABC-123 !!! et balise </div>\"\n",
    "texte_sale = nettoyer_texte_blacklist(texte_sale)\n",
    "print(texte_sale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "41ebc800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texte original:\n",
      "   Email testexamplecom  Prix â‚¬  Code ABC  et balise div\n",
      "\n",
      "-MÃ©thode whitelist (garde lettres+espaces):\n",
      "   Email testexamplecom  Prix   Code ABC  et balise div\n",
      "\n",
      "-MÃ©thode blacklist (supprime ponctuation+chiffres):\n",
      "   Email testexamplecom  Prix â‚¬  Code ABC  et balise div\n",
      "\n",
      "\n",
      "- USAGE EN ENTREPRISE:\n",
      "\n",
      "   + Whitelist: Quand vous savez exactement ce que vous voulez garder\n",
      "      Exemple: extraction de noms propres (uniquement lettres)\n",
      "\n",
      "   + Blacklist: Quand vous voulez juste retirer quelques Ã©lÃ©ments\n",
      "      Exemple: garder tout sauf la ponctuation\n",
      "\n",
      "   ğŸ¯ Avantage: Code trÃ¨s explicite et facile Ã  modifier\n",
      "      â†’ L'Ã©quipe comprend instantanÃ©ment ce qui est filtrÃ©\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTexte original:\\n   {texte_sale}\\n\")\n",
    "print(f\"-MÃ©thode whitelist (garde lettres+espaces):\\n   {nettoyer_texte_liste_filtrage(texte_sale)}\\n\")\n",
    "print(f\"-MÃ©thode blacklist (supprime ponctuation+chiffres):\\n   {nettoyer_texte_blacklist(texte_sale)}\\n\")\n",
    "\n",
    "print(\"\"\"\n",
    "- USAGE EN ENTREPRISE:\n",
    "   \n",
    "   + Whitelist: Quand vous savez exactement ce que vous voulez garder\n",
    "      Exemple: extraction de noms propres (uniquement lettres)\n",
    "   \n",
    "   + Blacklist: Quand vous voulez juste retirer quelques Ã©lÃ©ments\n",
    "      Exemple: garder tout sauf la ponctuation\n",
    "   \n",
    "   ğŸ¯ Avantage: Code trÃ¨s explicite et facile Ã  modifier\n",
    "      â†’ L'Ã©quipe comprend instantanÃ©ment ce qui est filtrÃ©\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "96c4b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Suppression des chiffres\n",
    "def supprimer_chiffres(texte):\n",
    "    \"\"\"Supprime tous les chiffres\"\"\"\n",
    "    return re.sub(r'\\d+', '', texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a78964f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Bonjour   tout le monde!!!  ğŸ˜€ \n",
      "    Ceci est un exemple de TEXTE Ã  nettoyer...   \n",
      "\n",
      "    Il contient des    espaces   multiples, des MAJUSCULES, \n",
      "    des chiffres comme  et , des URLs comme \n",
      "    et mÃªme des balises HTML comme ceci.\n",
      "\n",
      "    On peut aussi trouver des emails: \n",
      "    Et des Ã©moticÃ´nes :-)  :-(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texte_brut = supprimer_chiffres(texte_brut)\n",
    "print(texte_brut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4a37778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Suppression des Ã©mojis\n",
    "def supprimer_emojis(texte):\n",
    "    \"\"\"Supprime les Ã©mojis du texte\"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symboles & pictogrammes\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & symboles de carte\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # drapeaux\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1d4c89c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Bonjour   tout le monde!!!   \n",
      "    Ceci est un exemple de TEXTE Ã  nettoyer...   \n",
      "\n",
      "    Il contient des    espaces   multiples, des MAJUSCULES, \n",
      "    des chiffres comme  et , des URLs comme \n",
      "    et mÃªme des balises HTML comme ceci.\n",
      "\n",
      "    On peut aussi trouver des emails: \n",
      "    Et des Ã©moticÃ´nes :-)  :-(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remplacer texte (argument de la fonction par notre texte brute)\n",
    "texte_brut = supprimer_emojis(texte_brut)\n",
    "print(texte_brut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4576d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 Suppression des espaces multiples\n",
    "def supprimer_espaces_multiples(texte):\n",
    "    \"\"\"Remplace les espaces multiples par un seul espace\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', texte).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9bab5a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Bonjour   tout le monde!!!  ğŸ˜€ \n",
      "    Ceci est un exemple de TEXTE Ã  nettoyer...   \n",
      "\n",
      "    Il contient des    espaces   multiples, des MAJUSCULES, \n",
      "    des chiffres comme  et , des URLs comme \n",
      "    et mÃªme des balises HTML comme ceci.\n",
      "\n",
      "    On peut aussi trouver des emails: \n",
      "    Et des Ã©moticÃ´nes :-)  :-(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remplacer texte (argument de la fonction par notre texte brute)\n",
    "texte_brut = supprimer_espaces_multiples(texte_brut)\n",
    "print(text_brut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9ec6d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.7 Suppression des Ã©moticÃ´nes\n",
    "def supprimer_emoticones(texte):\n",
    "    \"\"\"Supprime les Ã©moticÃ´nes textuelles comme :-) :-(\"\"\"\n",
    "    emoticon_pattern = re.compile(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)')\n",
    "    return emoticon_pattern.sub(r'', texte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "23a09095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour tout le monde!!! Ceci est un exemple de TEXTE Ã  nettoyer... Il contient des espaces multiples, des MAJUSCULES, des chiffres comme et , des URLs comme et mÃªme des balises HTML comme ceci. On peut aussi trouver des emails: Et des Ã©moticÃ´nes  \n"
     ]
    }
   ],
   "source": [
    "texte_brut = supprimer_emoticones(texte_brut)\n",
    "print(texte_brut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "18d19807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8 Suppression de la ponctuation\n",
    "def supprimer_ponctuation(texte):\n",
    "    \"\"\"Supprime la ponctuation - MÃ©thode 1: translate()\"\"\"\n",
    "    return texte.translate(str.maketrans('', '', string.punctuation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e016b8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour tout le monde Ceci est un exemple de TEXTE Ã  nettoyer Il contient des espaces multiples des MAJUSCULES des chiffres comme et  des URLs comme et mÃªme des balises HTML comme ceci On peut aussi trouver des emails Et des Ã©moticÃ´nes  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remplacer texte (argument de la fonction par notre texte brute)\n",
    "texte_brut = supprimer_ponctuation(texte_brut)\n",
    "print(texte_brut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2d35b06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************************************\n",
      "MÃ‰THODES ALTERNATIVES POUR SUPPRIMER DES CARACTÃˆRES\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "# MÃ‰THODES ALTERNATIVES PROFESSIONNELLES\n",
    "print(\"\\n\" + \"*\"*60)\n",
    "print(\"MÃ‰THODES ALTERNATIVES POUR SUPPRIMER DES CARACTÃˆRES\")\n",
    "print(\"*\"*60)\n",
    "\n",
    "# MÃ©thode 2: List comprehension avec join (TRÃˆS UTILISÃ‰E EN PRODUCTION)\n",
    "def supprimer_ponctuation_v2(texte):\n",
    "    \"\"\"\n",
    "    MÃ©thode professionnelle: List comprehension + join\n",
    "    \n",
    "    Avantages:\n",
    "    - Plus lisible et pythonique\n",
    "    - Flexible: on peut ajouter des conditions complexes\n",
    "    - Performance excellente pour de gros volumes\n",
    "    \"\"\"\n",
    "    # On crÃ©e une liste de caractÃ¨res Ã  Ã©liminer\n",
    "    caracteres_a_eliminer = set(string.punctuation)\n",
    "    \n",
    "    # On garde seulement les caractÃ¨res qui ne sont PAS dans la liste\n",
    "    return ''.join([char for char in texte if char not in caracteres_a_eliminer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "65ba134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MÃ©thode 3: Regex (STANDARD DE L'INDUSTRIE)\n",
    "def supprimer_ponctuation_v3(texte):\n",
    "    \"\"\"\n",
    "    MÃ©thode regex: trÃ¨s puissante et standard\n",
    "    \n",
    "    Avantages:\n",
    "    - Pattern matching complexe possible\n",
    "    - Standard dans l'industrie NLP\n",
    "    - Une seule ligne de code\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9ac8be57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texte original: Bonjour! Comment Ã§a va?; On est lundi!, il faut Ãªtre en forme, pÃ³ur supportÃ©r le rÃ¨ste de la semaine, C'est gÃ©nial... #NLP @Python 2026\n",
      "\n",
      "ğŸ”§ MÃ©thode 1 (translate):         Bonjour Comment Ã§a va On est lundi il faut Ãªtre en forme pÃ³ur supportÃ©r le rÃ¨ste de la semaine Cest gÃ©nial NLP Python 2026\n",
      "ğŸ”§ MÃ©thode 2 (list comprehension): Bonjour Comment Ã§a va On est lundi il faut Ãªtre en forme pÃ³ur supportÃ©r le rÃ¨ste de la semaine Cest gÃ©nial NLP Python 2026\n",
      "ğŸ”§ MÃ©thode 3 (regex):              Bonjour Comment Ã§a va On est lundi il faut Ãªtre en forme pÃ³ur supportÃ©r le rÃ¨ste de la semaine Cest gÃ©nial NLP Python 2026\n",
      "\n",
      "======================================================================\n",
      "COMPARAISON DES MÃ‰THODES\n",
      "======================================================================\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ MÃ©thode                 â”‚ Performance  â”‚ LisibilitÃ©  â”‚ FlexibilitÃ©  â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ translate()             â”‚ â˜…â˜…â˜…â˜…â˜…        â”‚ â˜…â˜…â˜†â˜†â˜†       â”‚ â˜…â˜…â˜†â˜†â˜†        â”‚\n",
      "â”‚ list comprehension+join â”‚ â˜…â˜…â˜…â˜…â˜†        â”‚ â˜…â˜…â˜…â˜…â˜…       â”‚ â˜…â˜…â˜…â˜…â˜…        â”‚\n",
      "â”‚ regex (re.sub)          â”‚ â˜…â˜…â˜…â˜†â˜†        â”‚ â˜…â˜…â˜…â˜†â˜†       â”‚ â˜…â˜…â˜…â˜…â˜…        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "RECOMMANDATIONS :\n",
      "\n",
      "   âœ… translate(): Utilisez pour de trÃ¨s gros volumes (millions de docs)\n",
      "   âœ… list comprehension: Meilleur compromis lisibilitÃ©/performance\n",
      "   âœ… regex: Quand vous avez des patterns complexes Ã  matcher\n",
      "\n",
      "   En production NLP, on voit souvent list comprehension + join\n",
      "   car le code reste maintenable par toute l'Ã©quipe.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparaison\n",
    "texte_test_ponct = \"Bonjour! Comment Ã§a va?; On est lundi!, il faut Ãªtre en forme, pÃ³ur supportÃ©r le rÃ¨ste de la semaine, C'est gÃ©nial... #NLP @Python 2026\"\n",
    "print(f\"\\nTexte original: {texte_test_ponct}\\n\")\n",
    "\n",
    "resultat_v1 = supprimer_ponctuation(texte_test_ponct)\n",
    "resultat_v2 = supprimer_ponctuation_v2(texte_test_ponct)\n",
    "resultat_v3 = supprimer_ponctuation_v3(texte_test_ponct)\n",
    "\n",
    "print(\"ğŸ”§ MÃ©thode 1 (translate):        \", resultat_v1)\n",
    "print(\"ğŸ”§ MÃ©thode 2 (list comprehension):\", resultat_v2)\n",
    "print(\"ğŸ”§ MÃ©thode 3 (regex):             \", resultat_v3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARAISON DES MÃ‰THODES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ MÃ©thode                 â”‚ Performance  â”‚ LisibilitÃ©  â”‚ FlexibilitÃ©  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ translate()             â”‚ â˜…â˜…â˜…â˜…â˜…        â”‚ â˜…â˜…â˜†â˜†â˜†       â”‚ â˜…â˜…â˜†â˜†â˜†        â”‚\n",
    "â”‚ list comprehension+join â”‚ â˜…â˜…â˜…â˜…â˜†        â”‚ â˜…â˜…â˜…â˜…â˜…       â”‚ â˜…â˜…â˜…â˜…â˜…        â”‚\n",
    "â”‚ regex (re.sub)          â”‚ â˜…â˜…â˜…â˜†â˜†        â”‚ â˜…â˜…â˜…â˜†â˜†       â”‚ â˜…â˜…â˜…â˜…â˜…        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "RECOMMANDATIONS :\n",
    "   \n",
    "   âœ… translate(): Utilisez pour de trÃ¨s gros volumes (millions de docs)\n",
    "   âœ… list comprehension: Meilleur compromis lisibilitÃ©/performance\n",
    "   âœ… regex: Quand vous avez des patterns complexes Ã  matcher\n",
    "   \n",
    "   En production NLP, on voit souvent list comprehension + join\n",
    "   car le code reste maintenable par toute l'Ã©quipe.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4f5e40ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ AprÃ¨s suppression HTML:\n",
      "Bonjour   tout le monde!!!  ğŸ˜€  Ceci est un exemple de TEXTE Ã  nettoyer...   Il contient des    espac ...\n",
      "\n",
      "Texte aprÃ¨s nettoyage complet:\n",
      "Bonjour tout le monde Ceci est un exemple de TEXTE Ã  nettoyer Il contient des espaces multiples des MAJUSCULES des chiffres comme et des URLs comme mÃªme des balises HTML comme ceci On peut aussi trouver des emails Et des Ã©moticÃ´nes\n"
     ]
    }
   ],
   "source": [
    "new_texte_brut = \"Bonjour   tout le monde!!!  ğŸ˜€  Ceci est un exemple de TEXTE Ã  nettoyer...   Il contient des    espaces   multiples, des MAJUSCULES, des chiffres comme 2024 et 12345, des URLs comme https://example.com,et mÃªme des balises HTML <p>comme ceci</p>. On peut aussi trouver des emails: contact@example.com Et des Ã©moticÃ´nes :-)  :-(\"\n",
    "    \n",
    "\n",
    "# Application Ã©tape par Ã©tape\n",
    "texte_etape1 = supprimer_html(new_texte_brut)\n",
    "print(\"ğŸ”§ AprÃ¨s suppression HTML:\")\n",
    "print(texte_etape1[:100], \"...\\n\")\n",
    "\n",
    "texte_etape2 = supprimer_urls(texte_etape1)\n",
    "texte_etape3 = supprimer_emails(texte_etape2)\n",
    "texte_etape4 = supprimer_emojis(texte_etape3)\n",
    "texte_etape5 = supprimer_emoticones(texte_etape4)\n",
    "texte_etape6 = supprimer_chiffres(texte_etape5)\n",
    "texte_etape7 = supprimer_ponctuation(texte_etape6)\n",
    "texte_etape8 = supprimer_espaces_multiples(texte_etape7)\n",
    "\n",
    "print(\"Texte aprÃ¨s nettoyage complet:\")\n",
    "print(texte_etape8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6872e",
   "metadata": {},
   "source": [
    "PARTIE 2: NORMALISATION ET HARMONISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "49d3688a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARTIE 2: NORMALISATION ET HARMONISATION\n",
      "============================================================\n",
      "\n",
      "Texte en minuscules:\n",
      "bonjour tout le monde ceci est un exemple de texte Ã  nettoyer il contient des espaces multiples des majuscules des chiffres comme et des urls comme mÃªme des balises html comme ceci on peut aussi trouver des emails et des Ã©moticÃ´nes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARTIE 2: NORMALISATION ET HARMONISATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 2.1 Conversion en minuscules\n",
    "def mettre_en_minuscule(texte):\n",
    "    \"\"\"Convertit tout le texte en minuscules\"\"\"\n",
    "    return texte.lower()\n",
    "\n",
    "texte_minuscule = mettre_en_minuscule(texte_etape8)\n",
    "print(\"\\nTexte en minuscules:\")\n",
    "print(texte_minuscule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "277bd6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avant: I can't believe it's already 2026! We shouldn't waste time.\n",
      "AprÃ¨s dÃ©veloppement: I cannot believe it is already 2026! We should not waste time.\n",
      "\n",
      "Langue dÃ©tectÃ©e pour 'Bonjour, comment allez-vous aujourd'hui?': fr\n",
      "Langue dÃ©tectÃ©e pour 'Hello, how are you doing today?': en\n"
     ]
    }
   ],
   "source": [
    "# 2.2 Gestion des contractions (exemple en anglais)\n",
    "import contractions\n",
    "\n",
    "texte_anglais = \"I can't believe it's already 2026! We shouldn't waste time.\"\n",
    "texte_developpe = contractions.fix(texte_anglais)\n",
    "print(f\"\\nAvant: {texte_anglais}\")\n",
    "print(f\"AprÃ¨s dÃ©veloppement: {texte_developpe}\")\n",
    "\n",
    "# 2.3 DÃ©tection de langue\n",
    "from langdetect import detect, detect_langs\n",
    "\n",
    "texte_fr = \"Bonjour, comment allez-vous aujourd'hui?\"\n",
    "texte_en = \"Hello, how are you doing today?\"\n",
    "\n",
    "print(f\"\\nLangue dÃ©tectÃ©e pour '{texte_fr}': {detect(texte_fr)}\")\n",
    "print(f\"Langue dÃ©tectÃ©e pour '{texte_en}': {detect(texte_en)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a89bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/angelo-\n",
      "[nltk_data]     btma/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmentation en phrases:\n",
      "  Phrase 1: Le traitement du langage naturel est fascinant.\n",
      "  Phrase 2: Il permet aux machines de comprendre le langage humain.\n",
      "  Phrase 3: Nous allons apprendre ensemble!\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Segmentation en phrases\n",
    "\"\"\"\n",
    "nltk.download('punkt_tab')\n",
    "\"\"\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "texte_phrases = \"Le traitement du langage naturel est fascinant. Il permet aux machines de comprendre le langage humain. Nous allons apprendre ensemble!\"\n",
    "\n",
    "phrases = sent_tokenize(texte_phrases, language=\"english\")\n",
    "print(\"\\nSegmentation en phrases:\")\n",
    "for i, phrase in enumerate(phrases, 1):\n",
    "    print(f\"  Phrase {i}: {phrase.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "71d286e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¤ Phrase originale: c'est fini Anakin, j'ai l'avantage sur toi!\n",
      "ğŸ”¤ Tokens (mots): [\"c'est\", 'fini', 'Anakin', ',', \"j'ai\", \"l'avantage\", 'sur', 'toi', '!']\n",
      "ğŸ“Š Nombre de tokens: 9\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Tokenisation en mots\n",
    "phrase_exemple = \"c'est fini Anakin, j'ai l'avantage sur toi!\"\n",
    "tokens = word_tokenize(phrase_exemple, language='french')\n",
    "print(f\"\\nğŸ”¤ Phrase originale: {phrase_exemple}\")\n",
    "print(f\"ğŸ”¤ Tokens (mots): {tokens}\")\n",
    "print(f\"ğŸ“Š Nombre de tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a0bca002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1-grammes (mots individuels):\n",
      "[(\"c'est\",), ('fini',), ('anakin',), (',',), (\"j'ai\",)]\n",
      "\n",
      "2-grammes (paires de mots):\n",
      "[(\"c'est\", 'fini'), ('fini', 'anakin'), ('anakin', ','), (',', \"j'ai\"), (\"j'ai\", \"l'avantage\"), (\"l'avantage\", 'sur'), ('sur', 'toi'), ('toi', '!')]\n",
      "\n",
      "3-grammes (triplets de mots):\n",
      "[(\"c'est\", 'fini', 'anakin'), ('fini', 'anakin', ','), ('anakin', ',', \"j'ai\"), (',', \"j'ai\", \"l'avantage\"), (\"j'ai\", \"l'avantage\", 'sur'), (\"l'avantage\", 'sur', 'toi'), ('sur', 'toi', '!')]\n"
     ]
    }
   ],
   "source": [
    "# 3.3 N-grammes\n",
    "def generer_ngrams(texte, n):\n",
    "    \"\"\"GÃ©nÃ¨re les n-grammes d'un texte\"\"\"\n",
    "    tokens = word_tokenize(texte.lower(), language='french')\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return n_grams\n",
    "\n",
    "phrase_star_wars = \"c'est fini Anakin, j'ai l'avantage sur toi!\"\n",
    "\n",
    "# Unigrammes (1-gram)\n",
    "unigrammes = generer_ngrams(phrase_star_wars, 1)\n",
    "print(f\"\\n1-grammes (mots individuels):\")\n",
    "print(unigrammes[:5])\n",
    "\n",
    "# Bigrammes (2-grams)\n",
    "bigrammes = generer_ngrams(phrase_star_wars, 2)\n",
    "print(f\"\\n2-grammes (paires de mots):\")\n",
    "print(bigrammes)\n",
    "\n",
    "# Trigrammes (3-grams)\n",
    "trigrammes = generer_ngrams(phrase_star_wars, 3)\n",
    "print(f\"\\n3-grammes (triplets de mots):\")\n",
    "print(trigrammes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc95379",
   "metadata": {},
   "source": [
    "### **PARTIE 4: RÃ‰DUCTION LINGUISTIQUE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62baa65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'au, aux, avec, ce, ces, dans, de, des, du, elle, en, et, eux, il, ils, je, la, le, les, leur, lui, ma, mais, me, mÃªme, mes, moi, mon, ne, nos, notre, nous, on, ou, par, pas, pour, qu, que, qui, sa, se, ses, son, sur, ta, te, tes, toi, ton, tu, un, une, vos, votre, vous, c, d, j, l, Ã , m, n, s, t, y, Ã©tÃ©, Ã©tÃ©e, Ã©tÃ©es, Ã©tÃ©s, Ã©tant, Ã©tante, Ã©tants, Ã©tantes, suis, es, est, sommes, Ãªtes, sont, serai, seras, sera, serons, serez, seront, serais, serait, serions, seriez, seraient, Ã©tais, Ã©tait, Ã©tions, Ã©tiez, Ã©taient, fus, fut, fÃ»mes, fÃ»tes, furent, sois, soit, soyons, soyez, soient, fusse, fusses, fÃ»t, fussions, fussiez, fussent, ayant, ayante, ayantes, ayants, eu, eue, eues, eus, ai, as, avons, avez, ont, aurai, auras, aura, aurons, aurez, auront, aurais, aurait, aurions, auriez, auraient, avais, avait, avions, aviez, avaient, eut, eÃ»mes, eÃ»tes, eurent, aie, aies, ait, ayons, ayez, aient, eusse, eusses, eÃ»t, eussions, eussiez, eussent'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aprecu des stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25538ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARTIE 4: RÃ‰DUCTION LINGUISTIQUE\n",
      "============================================================\n",
      "\n",
      "ğŸš« Phrase originale: Le traitement du langage naturel est une branche de l'intelligence artificielle\n",
      "Tokens avec stop words: ['le', 'traitement', 'du', 'langage', 'naturel', 'est', 'une', 'branche', 'de', \"l'intelligence\", 'artificielle']\n",
      "âœ… Tokens sans stop words: ['traitement', 'langage', 'naturel', 'branche', \"l'intelligence\", 'artificielle']\n",
      "RÃ©duction: 11 â†’ 6 tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARTIE 4: RÃ‰DUCTION LINGUISTIQUE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 4.1 Suppression des mots vides (Stop words)\n",
    "stop_words_fr = set(stopwords.words('french'))\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "phrase_test = \"Le traitement du langage naturel est une branche de l'intelligence artificielle\"\n",
    "tokens_phrase = word_tokenize(phrase_test.lower(), language='french')\n",
    "\n",
    "tokens_filtres = [mot for mot in tokens_phrase if mot not in stop_words_fr]\n",
    "\n",
    "print(f\"\\nPhrase originale: {phrase_test}\")\n",
    "print(f\"Tokens avec stop words: {tokens_phrase}\")\n",
    "print(f\"Tokens sans stop words: {tokens_filtres}\")\n",
    "print(f\"RÃ©duction: {len(tokens_phrase)} â†’ {len(tokens_filtres)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4703975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************************************\n",
      "STEMMING - Extraction des racines\n",
      "************************************************************\n",
      "\n",
      "ğŸ“– Phrase de Stephen King:\n",
      "   Original: Ils marchaient dans l'obscuritÃ© pluvieuse comme des fantÃ´mes dÃ©charnÃ©s\n",
      "\n",
      "ğŸ”¨ AprÃ¨s stemming:\n",
      "   Tokens originaux: ['ils', 'marchaient', 'dans', \"l'obscuritÃ©\", 'pluvieuse', 'comme', 'des', 'fantÃ´mes', 'dÃ©charnÃ©s']\n",
      "   Stems (racines):  ['il', 'march', 'dan', \"l'obscur\", 'pluvieux', 'comm', 'de', 'fantÃ´m', 'dÃ©charn']\n",
      "\n",
      "ğŸ“Š Comparaison dÃ©taillÃ©e:\n",
      "   ils             â†’ il\n",
      "   marchaient      â†’ march\n",
      "   dans            â†’ dan\n",
      "   l'obscuritÃ©     â†’ l'obscur\n",
      "   pluvieuse       â†’ pluvieux\n",
      "   comme           â†’ comm\n",
      "   des             â†’ de\n",
      "   fantÃ´mes        â†’ fantÃ´m\n",
      "   dÃ©charnÃ©s       â†’ dÃ©charn\n",
      "\n",
      "ğŸ”¨ Stemming anglais (Porter):\n",
      "   walking      â†’ walk\n",
      "   walks        â†’ walk\n",
      "   walked       â†’ walk\n",
      "   walker       â†’ walker\n",
      "   console      â†’ consol\n",
      "   consoling    â†’ consol\n"
     ]
    }
   ],
   "source": [
    "# 4.2 Racinisation (Stemming)\n",
    "print(\"\\n\" + \"*\"*60)\n",
    "print(\"STEMMING - Extraction des racines\")\n",
    "print(\"*\"*60)\n",
    "\n",
    "# Stemmer franÃ§ais\n",
    "stemmer_fr = SnowballStemmer('french')\n",
    "\n",
    "phrase_king = \"Ils marchaient dans l'obscuritÃ© pluvieuse comme des fantÃ´mes dÃ©charnÃ©s\"\n",
    "tokens_king = word_tokenize(phrase_king.lower(), language='french')\n",
    "stems_fr = [stemmer_fr.stem(mot) for mot in tokens_king]\n",
    "\n",
    "print(f\"\\nPhrase de Stephen King:\")\n",
    "print(f\"   Original: {phrase_king}\")\n",
    "print(f\"\\nAprÃ¨s stemming:\")\n",
    "print(f\"   Tokens originaux: {tokens_king}\")\n",
    "print(f\"   Stems (racines):  {stems_fr}\")\n",
    "\n",
    "# Comparaison mot par mot\n",
    "print(\"\\nComparaison dÃ©taillÃ©e:\")\n",
    "for original, stem in zip(tokens_king, stems_fr):\n",
    "    if original != stem:\n",
    "        print(f\"   {original:15} â†’ {stem}\")\n",
    "\n",
    "# Stemmer anglais\n",
    "stemmer_en = PorterStemmer()\n",
    "mots_anglais = ['walking', 'walks', 'walked', 'walker', 'console', 'consoling']\n",
    "print(f\"\\nStemming anglais (Porter):\")\n",
    "for mot in mots_anglais:\n",
    "    print(f\"   {mot:12} â†’ {stemmer_en.stem(mot)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8a02a060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************************************\n",
      "LEMMATISATION - Forme canonique\n",
      "************************************************************\n",
      "\n",
      "Lemmatisation franÃ§aise (spaCy):\n",
      "   Les             â†’ le              (DET)\n",
      "   chevaux         â†’ cheval          (NOUN)\n",
      "   galopaient      â†’ galoper         (VERB)\n",
      "   les             â†’ le              (DET)\n",
      "   prairies        â†’ prairie         (NOUN)\n",
      "   verdoyantes     â†’ verdoyant       (ADJ)\n",
      "\n",
      "Lemmatisation anglaise (NLTK):\n",
      "   running      â†’ running      (nom) | run          (verbe)\n",
      "   ran          â†’ ran          (nom) | run          (verbe)\n",
      "   runs         â†’ run          (nom) | run          (verbe)\n",
      "   better       â†’ better       (nom) | better       (verbe)\n",
      "   best         â†’ best         (nom) | best         (verbe)\n",
      "   feet         â†’ foot         (nom) | feet         (verbe)\n",
      "   geese        â†’ goose        (nom) | geese        (verbe)\n",
      "\n",
      "Comparaison Stemming vs Lemmatisation:\n",
      "   courir       | Stem: cour       | Lemme: courir\n",
      "   courais      | Stem: cour       | Lemme: courai\n",
      "   couru        | Stem: couru      | Lemme: courir\n",
      "   courront     | Stem: courront   | Lemme: courir\n"
     ]
    }
   ],
   "source": [
    "# 4.3 Lemmatisation\n",
    "print(\"\\n\" + \"*\"*60)\n",
    "print(\"LEMMATISATION - Forme canonique\")\n",
    "print(\"*\"*60)\n",
    "\n",
    "# Lemmatisation avec spaCy (franÃ§ais)\n",
    "doc_fr = nlp_fr(\"Les chevaux galopaient rapidement dans les prairies verdoyantes\")\n",
    "print(\"\\nLemmatisation franÃ§aise (spaCy):\")\n",
    "for token in doc_fr:\n",
    "    if token.text != token.lemma_:\n",
    "        print(f\"   {token.text:15} â†’ {token.lemma_:15} ({token.pos_})\")\n",
    "\n",
    "# Lemmatisation avec NLTK (anglais)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "mots_test = ['running', 'ran', 'runs', 'better', 'best', 'feet', 'geese']\n",
    "print(\"\\nLemmatisation anglaise (NLTK):\")\n",
    "for mot in mots_test:\n",
    "    lemme = lemmatizer.lemmatize(mot)\n",
    "    lemme_v = lemmatizer.lemmatize(mot, pos='v')  # en tant que verbe\n",
    "    print(f\"   {mot:12} â†’ {lemme:12} (nom) | {lemme_v:12} (verbe)\")\n",
    "\n",
    "# Comparaison Stemming vs Lemmatisation\n",
    "print(\"\\nComparaison Stemming vs Lemmatisation:\")\n",
    "mots_comparaison = ['courir', 'courais', 'couru', 'courront']\n",
    "for mot in mots_comparaison:\n",
    "    stem = stemmer_fr.stem(mot)\n",
    "    lemme = nlp_fr(mot)[0].lemma_\n",
    "    print(f\"   {mot:12} | Stem: {stem:10} | Lemme: {lemme}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e792a",
   "metadata": {},
   "source": [
    "### **PARTIE 5: TECHNIQUES AVANCÃ‰ES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "79b93573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARTIE 5: TECHNIQUES AVANCÃ‰ES\n",
      "============================================================\n",
      "\n",
      "POS Tagging (Ã‰tiquetage grammatical):\n",
      "Texte: Le chat noir dort paisiblement sur le canapÃ© confortable.\n",
      "\n",
      "Mot             Lemme           POS        Tag dÃ©taillÃ©   \n",
      "------------------------------------------------------------\n",
      "Le              le              DET        DET            \n",
      "chat            chat            NOUN       NOUN           \n",
      "noir            noir            ADJ        ADJ            \n",
      "dort            dort            ADJ        ADJ            \n",
      "paisiblement    paisiblement    NOUN       NOUN           \n",
      "sur             sur             ADP        ADP            \n",
      "le              le              DET        DET            \n",
      "canapÃ©          canapÃ©          NOUN       NOUN           \n",
      "confortable     confortable     ADJ        ADJ            \n",
      ".               .               PUNCT      PUNCT          \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARTIE 5: TECHNIQUES AVANCÃ‰ES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 5.1 POS Tagging (Ã‰tiquetage morpho-syntaxique)\n",
    "texte_pos = \"Le chat noir dort paisiblement sur le canapÃ© confortable.\"\n",
    "doc_pos = nlp_fr(texte_pos)\n",
    "\n",
    "print(\"\\nPOS Tagging (Ã‰tiquetage grammatical):\")\n",
    "print(f\"Texte: {texte_pos}\\n\")\n",
    "print(f\"{'Mot':<15} {'Lemme':<15} {'POS':<10} {'Tag dÃ©taillÃ©':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for token in doc_pos:\n",
    "    print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10} {token.tag_:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "885e89e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reconnaissance d'EntitÃ©s NommÃ©es (NER):\n",
      "Texte: \n",
      "Apple Inc. a Ã©tÃ© fondÃ©e par Steve Jobs Ã  Cupertino en Californie. \n",
      "L'entreprise a lancÃ© l'iPhone en 2007, rÃ©volutionnant l'industrie des smartphones.\n",
      "Tim Cook est devenu CEO en 2011.\n",
      "\n",
      "\n",
      "EntitÃ©                    Type            Explication\n",
      "----------------------------------------------------------------------\n",
      "Apple Inc.                ORG             Companies, agencies, institutions, etc.\n",
      "Steve Jobs                PERSON          People, including fictional\n",
      "Californie                ORG             Companies, agencies, institutions, etc.\n",
      "2007                      DATE            Absolute or relative dates or periods\n",
      "rÃ©volutionnant l'industrie des smartphones ORG             Companies, agencies, institutions, etc.\n",
      "Tim Cook                  PERSON          People, including fictional\n",
      "2011                      DATE            Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "# 5.2 Reconnaissance d'EntitÃ©s NommÃ©es (NER)\n",
    "texte_ner = \"\"\"\n",
    "Apple Inc. a Ã©tÃ© fondÃ©e par Steve Jobs Ã  Cupertino en Californie. \n",
    "L'entreprise a lancÃ© l'iPhone en 2007, rÃ©volutionnant l'industrie des smartphones.\n",
    "Tim Cook est devenu CEO en 2011.\n",
    "\"\"\"\n",
    "\n",
    "doc_ner = nlp_en(texte_ner)\n",
    "\n",
    "print(\"\\nReconnaissance d'EntitÃ©s NommÃ©es (NER):\")\n",
    "print(f\"Texte: {texte_ner}\\n\")\n",
    "print(f\"{'EntitÃ©':<25} {'Type':<15} {'Explication'}\")\n",
    "print(\"-\" * 70)\n",
    "for ent in doc_ner.ents:\n",
    "    print(f\"{ent.text:<25} {ent.label_:<15} {spacy.explain(ent.label_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6846ca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š FrÃ©quence des mots dans le corpus:\n",
      "   le                        : 5 occurrence(s)\n",
      "   chat                      : 3 occurrence(s)\n",
      "   dort                      : 2 occurrence(s)\n",
      "   chien                     : 2 occurrence(s)\n",
      "   joue                      : 2 occurrence(s)\n",
      "   mange                     : 1 occurrence(s)\n",
      "   erreur_de_frappe_unique   : 1 occurrence(s)\n",
      "\n",
      "âœ‚ï¸ Mots conservÃ©s (frÃ©quence >= 2):\n",
      "   {'dort', 'chat', 'le', 'joue', 'chien'}\n"
     ]
    }
   ],
   "source": [
    "# 5.3 Gestion des mots Ã  faible frÃ©quence\n",
    "corpus_exemple = [\n",
    "    \"le chat dort\",\n",
    "    \"le chien joue\",\n",
    "    \"le chat mange\",\n",
    "    \"le chien dort\",\n",
    "    \"erreur_de_frappe_unique\",\n",
    "    \"le chat joue\"\n",
    "]\n",
    "\n",
    "# Comptage des frÃ©quences\n",
    "tous_les_mots = []\n",
    "for phrase in corpus_exemple:\n",
    "    tous_les_mots.extend(phrase.split())\n",
    "\n",
    "freq_mots = Counter(tous_les_mots)\n",
    "print(\"\\nFrÃ©quence des mots dans le corpus:\")\n",
    "for mot, freq in freq_mots.most_common():\n",
    "    print(f\"   {mot:<25} : {freq} occurrence(s)\")\n",
    "\n",
    "# Filtrage des mots rares\n",
    "seuil_frequence = 2\n",
    "mots_frequents = {mot for mot, freq in freq_mots.items() if freq >= seuil_frequence}\n",
    "print(f\"\\nMots conservÃ©s (frÃ©quence >= {seuil_frequence}):\")\n",
    "print(f\"   {mots_frequents}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33672b2",
   "metadata": {},
   "source": [
    "### **PARTIE 6: PIPELINE COMPLET DE PRÃ‰TRAITEMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9f34e41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARTIE 6: PIPELINE COMPLET\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARTIE 6: PIPELINE COMPLET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class PreprocesseurTexte:\n",
    "    \"\"\"\n",
    "    Classe pour prÃ©traiter du texte de maniÃ¨re complÃ¨te\n",
    "    \"\"\"\n",
    "    def __init__(self, langue='french'):\n",
    "        self.langue = langue\n",
    "        if langue == 'french':\n",
    "            self.stop_words = set(stopwords.words('french'))\n",
    "            self.stemmer = SnowballStemmer('french')\n",
    "            self.nlp = nlp_fr\n",
    "        else:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "            self.stemmer = PorterStemmer()\n",
    "            self.nlp = nlp_en\n",
    "    \n",
    "    def nettoyer(self, texte):\n",
    "        \"\"\"Nettoyage de base\"\"\"\n",
    "        texte = supprimer_html(texte)\n",
    "        texte = supprimer_urls(texte)\n",
    "        texte = supprimer_emails(texte)\n",
    "        texte = supprimer_emojis(texte)\n",
    "        texte = supprimer_emoticones(texte)\n",
    "        texte = supprimer_espaces_multiples(texte)\n",
    "        return texte\n",
    "    \n",
    "    def normaliser(self, texte):\n",
    "        \"\"\"Normalisation\"\"\"\n",
    "        texte = mettre_en_minuscule(texte)\n",
    "        return texte\n",
    "    \n",
    "    def tokeniser(self, texte):\n",
    "        \"\"\"Tokenisation\"\"\"\n",
    "        return word_tokenize(texte, language=self.langue)\n",
    "    \n",
    "    def supprimer_stopwords(self, tokens):\n",
    "        \"\"\"Suppression des stop words\"\"\"\n",
    "        return [token for token in tokens if token not in self.stop_words]\n",
    "    \n",
    "    def lemmatiser(self, tokens):\n",
    "        \"\"\"Lemmatisation\"\"\"\n",
    "        texte = ' '.join(tokens)\n",
    "        doc = self.nlp(texte)\n",
    "        return [token.lemma_ for token in doc]\n",
    "    \n",
    "    def pretraiter(self, texte, avec_lemmatisation=True):\n",
    "        \"\"\"Pipeline complet\"\"\"\n",
    "        # Ã‰tape 1: Nettoyage\n",
    "        texte = self.nettoyer(texte)\n",
    "        \n",
    "        # Ã‰tape 2: Normalisation\n",
    "        texte = self.normaliser(texte)\n",
    "        \n",
    "        # Ã‰tape 3: Tokenisation\n",
    "        tokens = self.tokeniser(texte)\n",
    "        \n",
    "        # Ã‰tape 4: Suppression stopwords\n",
    "        tokens = self.supprimer_stopwords(tokens)\n",
    "        \n",
    "        # Ã‰tape 5: Lemmatisation (optionnel)\n",
    "        if avec_lemmatisation:\n",
    "            tokens = self.lemmatiser(tokens)\n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f9af55b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Texte original:\n",
      "\n",
      "    Bonjour Ã  tous! ğŸ˜€ Ceci est un EXEMPLE de texte avec des URLs https://example.com,\n",
      "    des chiffres 12345, et de la ponctuation!!! Nous allons le prÃ©traiter complÃ¨tement.\n",
      "\n",
      "\n",
      "âœ¨ RÃ©sultat final aprÃ¨s prÃ©traitement complet:\n",
      "['bonjour', 'tout', '!', 'ceci', 'exemple', 'texte', 'urls', 'chiffre', '12345', ',', 'ponctuation', '!', '!', '!', 'allon', 'prÃ©traiter', 'complÃ¨tement', '.']\n"
     ]
    }
   ],
   "source": [
    "#Test du pipeline\n",
    "texte_test = \"\"\"\n",
    "    Bonjour Ã  tous! ğŸ˜€ Ceci est un EXEMPLE de texte avec des URLs https://example.com,\n",
    "    des chiffres 12345, et de la ponctuation!!! Nous allons le prÃ©traiter complÃ¨tement.\n",
    "\"\"\"\n",
    "\n",
    "preprocesseur = PreprocesseurTexte(langue='french')\n",
    "\n",
    "print(\"\\nğŸ“ Texte original:\")\n",
    "print(texte_test)\n",
    "\n",
    "tokens_finaux = preprocesseur.pretraiter(texte_test)\n",
    "\n",
    "print(\"\\nâœ¨ RÃ©sultat final aprÃ¨s prÃ©traitement complet:\")\n",
    "print(tokens_finaux)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b664d",
   "metadata": {},
   "source": [
    "### **PARTIE 7: PRÃ‰TRAITEMENT SPÃ‰CIFIQUE AU DEEP LEARNING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "00c06b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARTIE 7: PRÃ‰TRAITEMENT POUR LE DEEP LEARNING\n",
      "============================================================\n",
      "\n",
      "CONTEXTE:\n",
      "Les rÃ©seaux de neurones profonds ont des contraintes strictes:\n",
      "- Les donnÃ©es doivent avoir la MÃŠME FORME (shape)\n",
      "- Les batches nÃ©cessitent des sÃ©quences de longueur identique\n",
      "- La mÃ©moire GPU est limitÃ©e\n",
      "\n",
      "Nous allons voir les techniques essentielles pour prÃ©parer\n",
      "du texte pour les modÃ¨les de Deep Learning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARTIE 7: PRÃ‰TRAITEMENT POUR LE DEEP LEARNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "CONTEXTE:\n",
    "Les rÃ©seaux de neurones profonds ont des contraintes strictes:\n",
    "- Les donnÃ©es doivent avoir la MÃŠME FORME (shape)\n",
    "- Les batches nÃ©cessitent des sÃ©quences de longueur identique\n",
    "- La mÃ©moire GPU est limitÃ©e\n",
    "\n",
    "Nous allons voir les techniques essentielles pour prÃ©parer\n",
    "du texte pour les modÃ¨les de Deep Learning.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7dbe7d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5498.84s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.13/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.13/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in ./.venv/lib/python3.13/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.13/site-packages (from tensorflow) (2.0.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.76.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.13.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from tensorflow) (2.4.1)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.4-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pillow (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached pillow-12.1.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.18.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in ./.venv/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m620.8/620.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m  \u001b[33m0:02:09\u001b[0mm0:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.76.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.4-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.15.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.13.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (414 kB)\n",
      "Using cached pillow-12.1.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wheel, werkzeug, termcolor, tensorboard-data-server, protobuf, pillow, optree, opt_einsum, ml_dtypes, mdurl, markdown, h5py, grpcio, google_pasta, gast, absl-py, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m25/25\u001b[0m [tensorflow]5\u001b[0m [tensorflow]-py]a-server]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.12.19 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 keras-3.13.1 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.4 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 pillow-12.1.0 protobuf-6.33.4 rich-14.2.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.3.0 werkzeug-3.1.5 wheel-0.45.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "abb1abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "7.1 PADDING - Uniformiser la longueur des sÃ©quences\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 13:26:41.122506: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-18 13:26:41.582256: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-18 13:26:44.069631: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SÃ©quences originales (longueurs variables):\n",
      "   SÃ©quence 1: [1, 2, 3, 4, 5] (longueur: 5)\n",
      "   SÃ©quence 2: [6, 7, 8] (longueur: 3)\n",
      "   SÃ©quence 3: [9, 10, 11, 12, 13, 14, 15] (longueur: 7)\n"
     ]
    }
   ],
   "source": [
    "# 7.1 PADDING (Remplissage)\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"7.1 PADDING - Uniformiser la longueur des sÃ©quences\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Alternative sans Keras:\n",
    "from itertools import zip_longest\n",
    "\n",
    "# Exemple: 3 phrases de longueurs diffÃ©rentes (dÃ©jÃ  tokenisÃ©es en indices)\n",
    "sequences = [\n",
    "    [1, 2, 3, 4, 5],           # 5 mots\n",
    "    [6, 7, 8],                  # 3 mots\n",
    "    [9, 10, 11, 12, 13, 14, 15] # 7 mots\n",
    "]\n",
    "\n",
    "print(\"\\nSÃ©quences originales (longueurs variables):\")\n",
    "for i, seq in enumerate(sequences, 1):\n",
    "    print(f\"   SÃ©quence {i}: {seq} (longueur: {len(seq)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "65386021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MÃ©thode 1 - POST-PADDING (Keras):\n",
      "   Padding Ã  la FIN des sÃ©quences:\n",
      "[[ 1  2  3  4  5  0  0]\n",
      " [ 6  7  8  0  0  0  0]\n",
      " [ 9 10 11 12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "# MÃ©thode 1: Padding avec Keras (POST-PADDING)\n",
    "padded_post = pad_sequences(sequences, padding='post', value=0)\n",
    "print(\"\\nMÃ©thode 1 - POST-PADDING (Keras):\")\n",
    "print(\"   Padding Ã  la FIN des sÃ©quences:\")\n",
    "print(padded_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9f650ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MÃ©thode 2 - PRE-PADDING (Keras):\n",
      "   Padding au DÃ‰BUT des sÃ©quences:\n",
      "[[ 0  0  1  2  3  4  5]\n",
      " [ 0  0  0  0  6  7  8]\n",
      " [ 9 10 11 12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "# MÃ©thode 2: Padding avec Keras (PRE-PADDING)\n",
    "padded_pre = pad_sequences(sequences, padding='pre', value=0)\n",
    "print(\"\\nMÃ©thode 2 - PRE-PADDING (Keras):\")\n",
    "print(\"   Padding au DÃ‰BUT des sÃ©quences:\")\n",
    "print(padded_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "baff3bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MÃ©thode 3 - Troncature MANUELLE:\n",
      "   [DATA_START] OBJET: \n",
      "\n",
      "STRATÃ‰GIES DE TRONCATURE:\n",
      "\n",
      "    TRONCATURE POST (garder le dÃ©but):\n",
      "      âœ… Pour les articles, documents (intro importante)\n",
      "      âœ… Standard pour la plupart des tÃ¢ches\n",
      "\n",
      "    TRONCATURE PRE (garder la fin):\n",
      "      âœ… Pour les conversations (contexte rÃ©cent)\n",
      "      âœ… Chatbots, dialogue\n",
      "\n",
      "    TRONCATURE INTELLIGENTE:\n",
      "      âœ… Garder dÃ©but + fin, supprimer le milieu\n",
      "      âœ… RÃ©sumÃ©s, questions-rÃ©ponses\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MÃ©thode 2: Troncature manuelle (Plus de contrÃ´le)\n",
    "longue_sequence = \"\"\"[DATA_START] OBJET: Rapport d'analyse sur l'Ã©volution des architectures de rÃ©seaux de neurones (2024-2026) ğŸ“ˆ\n",
    "\n",
    "INTRODUCTION : L'intelligence artificielle a franchi une Ã©tape cruciale avec l'avÃ¨nement des Transformers. Contrairement aux anciens rÃ©seaux rÃ©currents (RNN) comme les LSTM qui traitaient l'information de maniÃ¨re sÃ©quentielle pas Ã  pas, le mÃ©canisme d'attention permet de regarder l'ensemble de la phrase simultanÃ©ment. ğŸ§  Cependant, cette puissance vient avec un coÃ»t computationnel Ã©levÃ©, notamment une complexitÃ© quadratique par rapport Ã  la longueur de la sÃ©quence d'entrÃ©e.\n",
    "\n",
    "CORPS DU TEXTE : Dans le cadre de notre projet de Deep Learning, nous avons observÃ© que la gestion de la mÃ©moire GPU devient critique lorsque nous dÃ©passons une certaine limite de tokens. âš ï¸ Imaginez un utilisateur qui soumet un document entier de 5000 mots Ã  un modÃ¨le limitÃ© Ã  512 tokens. Sans une fonction de tronquage efficace, le systÃ¨me risque de planter ou de gÃ©nÃ©rer une erreur de type \"OutOfMemoryError\".\n",
    "\n",
    "Voici quelques points techniques Ã  tester avec votre fonction : 1. La perte d'information : Si on tronque Ã  la fin (post-truncation), on perd souvent la conclusion ou les tags de fin. ğŸ”š 2. Le contexte initial : Si on tronque au dÃ©but (pre-truncation), on perd le sujet principal de l'article. ğŸ” 3. Les caractÃ¨res spÃ©ciaux : Comment la fonction rÃ©agit-elle face Ã  des Ã©mojis complexes comme ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ou des sÃ©quences de ponctuation rÃ©pÃ©tÃ©es ......... ?\n",
    "\n",
    "RECOMMANDATIONS : Il est conseillÃ© d'utiliser une stratÃ©gie de \"Sliding Window\" (fenÃªtre glissante) si le texte est trop long, plutÃ´t qu'un tronquage brutal. Mais pour votre exercice de base, nous allons simplement couper la sÃ©quence Ã  une longueur fixe, par exemple MAX_LEN = 128 ou 256.\n",
    "\n",
    "NOTES DE FIN : Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. ğŸ›‘ [END_OF_STRING]\"\"\"\n",
    "def tronquer_sequence(sequence, max_len, position='post'):\n",
    "    \"\"\"\n",
    "    Troncature personnalisÃ©e\n",
    "    \"\"\"\n",
    "    if len(sequence) <= max_len:\n",
    "        return sequence\n",
    "    \n",
    "    if position == 'post':\n",
    "        # Garder le dÃ©but\n",
    "        return sequence[:max_len]\n",
    "    else:  # pre\n",
    "        # Garder la fin\n",
    "        return sequence[-max_len:]\n",
    "\n",
    "tronquee_manuel = tronquer_sequence(longue_sequence, 20, position='post')\n",
    "print(f\"\\nMÃ©thode 3 - Troncature MANUELLE:\")\n",
    "print(f\"   {tronquee_manuel}\")\n",
    "\n",
    "print(\"\"\"\n",
    "STRATÃ‰GIES DE TRONCATURE:\n",
    "\n",
    "    TRONCATURE POST (garder le dÃ©but):\n",
    "      âœ… Pour les articles, documents (intro importante)\n",
    "      âœ… Standard pour la plupart des tÃ¢ches\n",
    "   \n",
    "    TRONCATURE PRE (garder la fin):\n",
    "      âœ… Pour les conversations (contexte rÃ©cent)\n",
    "      âœ… Chatbots, dialogue\n",
    "   \n",
    "    TRONCATURE INTELLIGENTE:\n",
    "      âœ… Garder dÃ©but + fin, supprimer le milieu\n",
    "      âœ… RÃ©sumÃ©s, questions-rÃ©ponses\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c09de479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ COMBINAISON PADDING + TRONCATURE:\n",
      "   Longueur cible: 5\n",
      "\n",
      "   Avant: [[1, 2], [3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14]]\n",
      "\n",
      "   AprÃ¨s:\n",
      "[[ 1  2  0  0  0]\n",
      " [ 3  4  5  6  7]\n",
      " [ 8  9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "# Combinaison Padding + Troncature\n",
    "print(\"\\nğŸ”„ COMBINAISON PADDING + TRONCATURE:\")\n",
    "sequences_mixtes = [\n",
    "    [1, 2],                      # Trop courte â†’ Padding\n",
    "    [3, 4, 5, 6, 7],             # Bonne longueur\n",
    "    [8, 9, 10, 11, 12, 13, 14]   # Trop longue â†’ Troncature\n",
    "]\n",
    "\n",
    "MAX_LEN = 5\n",
    "sequences_uniformes = pad_sequences(sequences_mixtes, maxlen=MAX_LEN, \n",
    "                                   padding='post', truncating='post', value=0)\n",
    "\n",
    "print(f\"   Longueur cible: {MAX_LEN}\")\n",
    "print(f\"\\n   Avant: {sequences_mixtes}\")\n",
    "print(f\"\\n   AprÃ¨s:\")\n",
    "print(sequences_uniformes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6eab0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "7.3 AUGMENTATION DE DONNÃ‰ES - Enrichir le dataset\n",
      "------------------------------------------------------------\n",
      "\n",
      "OBJECTIF:\n",
      "CrÃ©er artificiellement plus de donnÃ©es d'entraÃ®nement pour:\n",
      "- AmÃ©liorer la robustesse du modÃ¨le\n",
      "- RÃ©duire l'overfitting\n",
      "- Simuler des variations naturelles du langage\n",
      "\n",
      "\n",
      "Texte original: 'Le chat noir dort paisiblement sur le canapÃ©'\n"
     ]
    }
   ],
   "source": [
    "# 7.3 AUGMENTATION DE DONNÃ‰ES TEXTUELLES\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"7.3 AUGMENTATION DE DONNÃ‰ES - Enrichir le dataset\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "OBJECTIF:\n",
    "CrÃ©er artificiellement plus de donnÃ©es d'entraÃ®nement pour:\n",
    "- AmÃ©liorer la robustesse du modÃ¨le\n",
    "- RÃ©duire l'overfitting\n",
    "- Simuler des variations naturelles du langage\n",
    "\"\"\")\n",
    "\n",
    "texte_original = \"Le chat noir dort paisiblement sur le canapÃ©\"\n",
    "print(f\"\\nTexte original: '{texte_original}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b552da4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Technique 1: SUBSTITUTION DE SYNONYMES\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    }
   ],
   "source": [
    "# Technique 1: SUBSTITUTION DE SYNONYMES\n",
    "print(\"\\n\" + \"â”€\"*60)\n",
    "print(\"Technique 1: SUBSTITUTION DE SYNONYMES\")\n",
    "print(\"â”€\"*60)\n",
    "\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def obtenir_synonymes(mot, langue='fra'):\n",
    "    \"\"\"Obtenir les synonymes d'un mot\"\"\"\n",
    "    synonymes = []\n",
    "    for syn in wordnet.synsets(mot, lang=langue):\n",
    "        for lemma in syn.lemmas(lang=langue):\n",
    "            if lemma.name() != mot:\n",
    "                synonymes.append(lemma.name().replace('_', ' '))\n",
    "    return list(set(synonymes))\n",
    "\n",
    "# Version simplifiÃ©e avec dictionnaire manuel\n",
    "dictionnaire_synonymes = {\n",
    "    'chat': ['fÃ©lin', 'minet', 'matou'],\n",
    "    'noir': ['sombre', 'obscur', 'tÃ©nÃ©breux'],\n",
    "    'dort': ['sommeille', 'repose', 'roupille'],\n",
    "    'paisiblement': ['calmement', 'tranquillement', 'sereinement'],\n",
    "    'canapÃ©': ['sofa', 'divan', 'banquette']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "eaf907dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Variations gÃ©nÃ©rÃ©es:\n",
      "   Variation 1: 'le chat noir dort paisiblement sur le sofa'\n",
      "   Variation 2: 'le chat obscur sommeille calmement sur le divan'\n",
      "   Variation 3: 'le chat sombre dort paisiblement sur le canapÃ©'\n"
     ]
    }
   ],
   "source": [
    "def augmenter_par_synonymes(texte, dico_synonymes, prob=0.3):\n",
    "    \"\"\"\n",
    "    Remplace alÃ©atoirement des mots par leurs synonymes\n",
    "    prob: probabilitÃ© de remplacement de chaque mot\n",
    "    \"\"\"\n",
    "    mots = texte.lower().split()\n",
    "    nouveaux_mots = []\n",
    "    \n",
    "    for mot in mots:\n",
    "        if mot in dico_synonymes and random.random() < prob:\n",
    "            # Remplacer par un synonyme alÃ©atoire\n",
    "            synonyme = random.choice(dico_synonymes[mot])\n",
    "            nouveaux_mots.append(synonyme)\n",
    "        else:\n",
    "            nouveaux_mots.append(mot)\n",
    "    \n",
    "    return ' '.join(nouveaux_mots)\n",
    "\n",
    "# GÃ©nÃ©rer 3 variations\n",
    "print(\"\\nğŸ”„ Variations gÃ©nÃ©rÃ©es:\")\n",
    "for i in range(3):\n",
    "    variation = augmenter_par_synonymes(texte_original, dictionnaire_synonymes, prob=0.5)\n",
    "    print(f\"   Variation {i+1}: '{variation}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "30e9881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Original:        'Le chat noir dort paisiblement sur le canapÃ©'\n",
      "   RÃ©tro-traduit:   'Le chat de couleur noire repose tranquillement sur le sofa'\n",
      "\n",
      "ğŸ”§ EN PRODUCTION:\n",
      "\n",
      "   from googletrans import Translator\n",
      "   translator = Translator()\n",
      "\n",
      "   # FR â†’ EN â†’ FR\n",
      "   en = translator.translate(texte, src='fr', dest='en').text\n",
      "   fr_retour = translator.translate(en, src='en', dest='fr').text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simulation du processus (en vrai, on utiliserait une API de traduction)\n",
    "def simuler_retro_traduction(texte):\n",
    "    \"\"\"\n",
    "    Simulation de la rÃ©tro-traduction\n",
    "    En production: utilisez Google Translate API, DeepL, etc.\n",
    "    \"\"\"\n",
    "    # Simulation de variations syntaxiques\n",
    "    variations = {\n",
    "        \"Le chat noir dort paisiblement sur le canapÃ©\": \n",
    "            \"Le chat de couleur noire repose tranquillement sur le sofa\",\n",
    "        \"J'adore programmer en Python\":\n",
    "            \"J'aime beaucoup coder avec Python\",\n",
    "        \"La voiture est trÃ¨s rapide\":\n",
    "            \"L'automobile est extrÃªmement vÃ©loce\"\n",
    "    }\n",
    "    return variations.get(texte, texte)\n",
    "\n",
    "texte_retro = simuler_retro_traduction(texte_original)\n",
    "print(f\"\\n   Original:        '{texte_original}'\")\n",
    "print(f\"   RÃ©tro-traduit:   '{texte_retro}'\")\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ”§ EN PRODUCTION:\n",
    "   \n",
    "   from googletrans import Translator\n",
    "   translator = Translator()\n",
    "   \n",
    "   # FR â†’ EN â†’ FR\n",
    "   en = translator.translate(texte, src='fr', dest='en').text\n",
    "   fr_retour = translator.translate(en, src='en', dest='fr').text\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "df5dce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Technique 3: INSERTION/SUPPRESSION ALÃ‰ATOIRE\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    }
   ],
   "source": [
    "# Technique 3: INSERTION/SUPPRESSION ALÃ‰ATOIRE\n",
    "print(\"\\n\" + \"â”€\"*60)\n",
    "print(\"Technique 3: INSERTION/SUPPRESSION ALÃ‰ATOIRE\")\n",
    "print(\"â”€\"*60)\n",
    "\n",
    "def insertion_aleatoire(texte, n_insertions=1):\n",
    "    \"\"\"\n",
    "    InsÃ¨re alÃ©atoirement des mots du texte Ã  des positions alÃ©atoires\n",
    "    Simule des rÃ©pÃ©titions naturelles\n",
    "    \"\"\"\n",
    "    mots = texte.split()\n",
    "    for _ in range(n_insertions):\n",
    "        # Choisir un mot au hasard\n",
    "        mot_a_inserer = random.choice(mots)\n",
    "        # Position alÃ©atoire\n",
    "        position = random.randint(0, len(mots))\n",
    "        mots.insert(position, mot_a_inserer)\n",
    "    \n",
    "    return ' '.join(mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b54686b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppression_aleatoire(texte, prob=0.1):\n",
    "    \"\"\"\n",
    "    Supprime alÃ©atoirement des mots (simule oublis/erreurs)\n",
    "    \"\"\"\n",
    "    mots = texte.split()\n",
    "    # Garder au moins 1 mot\n",
    "    if len(mots) == 1:\n",
    "        return texte\n",
    "    \n",
    "    mots_gardes = [mot for mot in mots if random.random() > prob]\n",
    "    \n",
    "    # S'assurer qu'on garde au moins 1 mot\n",
    "    if not mots_gardes:\n",
    "        mots_gardes = [random.choice(mots)]\n",
    "    \n",
    "    return ' '.join(mots_gardes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "30991279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def echange_aleatoire(texte, n_echanges=1):\n",
    "    \"\"\"\n",
    "    Ã‰change alÃ©atoirement des mots adjacents\n",
    "    Simule des erreurs de frappe ou de syntaxe\n",
    "    \"\"\"\n",
    "    mots = texte.split()\n",
    "    for _ in range(n_echanges):\n",
    "        if len(mots) >= 2:\n",
    "            idx = random.randint(0, len(mots) - 2)\n",
    "            mots[idx], mots[idx + 1] = mots[idx + 1], mots[idx]\n",
    "    \n",
    "    return ' '.join(mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8926fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Original:     'Le chat noir dort paisiblement sur le canapÃ©'\n",
      "   Insertion:    'Le chat noir noir dort paisiblement sur le canapÃ© noir'\n",
      "   Suppression:  'chat dort paisiblement le canapÃ©'\n",
      "   Ã‰change:      'Le chat noir dort paisiblement sur canapÃ© le'\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n   Original:     '{texte_original}'\")\n",
    "print(f\"   Insertion:    '{insertion_aleatoire(texte_original, n_insertions=2)}'\")\n",
    "print(f\"   Suppression:  '{suppression_aleatoire(texte_original, prob=0.2)}'\")\n",
    "print(f\"   Ã‰change:      '{echange_aleatoire(texte_original, n_echanges=1)}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
